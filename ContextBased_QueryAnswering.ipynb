{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do\n",
    "1. Coherence Score and Optimal Topic Number\n",
    "2. Scores for LSA and LDA\n",
    "3. GUI (Maybe Anvil or Streamlit if time permits)\n",
    "4. Add unigrams and bigrams to clean and clean again (with more samples) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SANHAPRI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "\n",
    "import contractions\n",
    "\n",
    "from string import digits\n",
    "\n",
    "import re\n",
    "\n",
    "import pymongo\n",
    "\n",
    "import gensim\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from bson.objectid import ObjectId\n",
    "\n",
    "from tqdm import tqdm\n",
    "from operator import itemgetter\n",
    "import time\n",
    "import os, multiprocessing, time\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import operator\n",
    "import warnings\n",
    "\n",
    "from gensim.models import LsiModel\n",
    "from collections import defaultdict\n",
    "from configparser import ConfigParser\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "nltk.download('stopwords')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "\n",
    "    res = str(text).translate(remove_digits)\n",
    "\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    text = strip_html_tags(text)\n",
    "    \n",
    "    cont = [contractions.fix(tok) for tok in text.split()]\n",
    "    \n",
    "    text = ' '.join([word for word in cont if not word.isdigit()])\n",
    "\n",
    "    text = re.sub(' +',' ',res)\n",
    "        \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    clean = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    \n",
    "    \n",
    "    return ' '.join(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_tags(text):\n",
    "    \"\"\"remove html tags from text\"\"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text(separator=\" \")\n",
    "    return stripped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs_per_topic(model):\n",
    "    all_topics = model.print_topics()\n",
    "    \n",
    "    docs_per_topic = [[] for _ in all_topics]\n",
    "    \n",
    "    for doc_id, doc_bow in enumerate(corpus):\n",
    "        doc_topics = model.get_document_topics(doc_bow)\n",
    "        \n",
    "        for topic_id, score in doc_topics:\n",
    "            docs_per_topic[topic_id].append((doc_id, score))\n",
    "    return docs_per_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_lda(q_text,docs_per_topic):\n",
    "    unseen_doc = dictionary.doc2bow(clean_text(q_text).split(\" \"))\n",
    "    vector = lda_model[unseen_doc]\n",
    "    \n",
    "    vector.sort(key=itemgetter(1), reverse=True)\n",
    "    \n",
    "    topic_id = vector[0][0]\n",
    "    \n",
    "    doc_id = docs_per_topic[topic_id][0][0]\n",
    "    \n",
    "    answer = ques_col.find_one({ \"clean_aa\": ' '.join(documents[doc_id])})['accepted_answer']\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_lsa(q_text,docs_per_topic):\n",
    "    unseen_doc=clean_text(q_text).split(\" \")\n",
    "    \n",
    "    new_vec = dictionary.doc2bow(unseen_doc)\n",
    "    vec_bow_tfidf = tfidf[new_vec]\n",
    "    vector = lsi[vec_bow_tfidf]\n",
    "   \n",
    "\n",
    "    vector.sort(key=itemgetter(1), reverse=True)\n",
    "    \n",
    "    topic_id = vector[0][0]\n",
    "    \n",
    "    doc_id = docs_per_topic[topic_id][0][0]\n",
    "    \n",
    "    answer = ques_col.find_one({ \"clean_aa\": ' '.join(documents[doc_id])})['accepted_answer']\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lsa_topics(svd, terms, n_components, n_out = 7, n_weight = 5, topic = None):\n",
    "    if topic is None:\n",
    "        for k in range(n_components):\n",
    "            idx = {i:abs(j) for i, j in enumerate(svd.components_[k])}\n",
    "            sorted_idx = sorted(idx.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            weight = np.mean([ item[1] for item in sorted_idx[0:n_weight] ])\n",
    "            print(\"T%s)\" % k, end =' ')\n",
    "            for item in sorted_idx[0:n_out-1]:\n",
    "                print( \" %0.3f*%s\"  % (item[1] , terms[item[0]]) , end=' ')\n",
    "            print()\n",
    "    else:\n",
    "        m = max(svd.components_[topic])\n",
    "        idx = {i:abs(j) for i, j in enumerate(svd.components_[topic])}\n",
    "        sorted_idx = sorted(idx.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        weight = np.mean([ item[1] for item in sorted_idx[0:n_weight] ])\n",
    "        print(\"* T %s) weight: %0.2f\" % (topic, weight), end=' ')\n",
    "        for item in sorted_idx[0:n_out-1]:\n",
    "            print( \" %0.3f*%s\"  % (item[1] , terms[item[0]]) , end=' ')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(svdX, y_pred, centers):\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    f, ax1 = plt.subplots(1, 1, figsize=( 16, 8), facecolor='white')\n",
    "    ax1.set_xlabel(\"\")\n",
    "    ax1.set_ylabel(\"\")\n",
    "    ax1.set_title(\"K-Means\")\n",
    "    # Only plots the first 2 dimensions of the svdX matrix\n",
    "    ax1.scatter(svdX[:,0], svdX[:,1], c=y_pred, cmap=plt.cm.Paired, s=45)\n",
    "    ax1.scatter(centers[:, 0], centers[:, 1], marker='o', c=\"black\", alpha=1, s=150)\n",
    "    ax1.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starter Code & Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "db = conn['BDMSEndSem']\n",
    "\n",
    "ques_col = db.get_collection('query_answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samp = 1000\n",
    "\n",
    "D = ques_col.find({}, no_cursor_timeout=True).limit(n_samp)\n",
    "\n",
    "for i, doc in tqdm(enumerate(D)):\n",
    "    \n",
    "    update = {'clean_aa': clean_text(doc['accepted_answer']),\n",
    "          'clean_qt': clean_text(doc['question_title']),\n",
    "          'clean_qb': clean_text(doc['question_body']),\n",
    "          'clean_tg': clean_text(doc['tags'])}\n",
    "    \n",
    "    ques_col.update_one(\n",
    "        {'_id': ObjectId(doc['_id'])},\n",
    "        { \"$set\":update })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_frequency = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samp = 10000\n",
    "docs = ques_col.find({}, no_cursor_timeout=True).limit(n_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "for doc in docs:\n",
    "    for token in doc['clean_aa'].split(\" \"):\n",
    "        token_frequency[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [ [token for token in doc['clean_aa'].split(\" \") if token_frequency[token] > 1]\n",
    "                for doc in docs  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(documents)\n",
    "dictionary.compactify()\n",
    "dictionary.save('stack_of.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "corpora.MmCorpus.serialize('stack_of.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary.load(\"stack_of.dict\")\n",
    "lda_model=models.LdaModel.load('lda_model.lda')\n",
    "corpus = corpora.MmCorpus(\"stack_of.mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bow_doc_4310 = corpus[4310]\n",
    "# for i in range(len(bow_doc_4310)):\n",
    "#     print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "#                                                dictionary[bow_doc_4310[i][0]], bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = models.TfidfModel(corpus)\n",
    "# corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "# for doc in corpus_tfidf:\n",
    "#     pprint(doc)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=100, id2word=dictionary, passes=20, workers=8)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf[corpus[6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_document = 'Applying CSS to Datatables that has NEXT Page'\n",
    "\n",
    "bow_vector = dictionary.doc2bow(clean_text(unseen_document).split(\" \"))\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_document = 'How to add a smooth transition effect'\n",
    "\n",
    "bow_vector = dictionary.doc2bow(clean_text(unseen_document).split(\" \"))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.show_document(9, topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.get_document_topics(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_data =  gensimvis.prepare(lda_model, bow_corpus, dictionary)\n",
    "pyLDAvis.display(followers_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_per_topic=get_docs_per_topic(lda_model)\n",
    "for i in range(10):\n",
    "    docs_per_topic[i].sort(reverse=True,key=itemgetter(1))\n",
    "get_answer_lda('How to add a smooth transition in HTML with CSS?',docs_per_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(corpus, smartirs='npu')\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_model = gensim.models.LsiModel(corpus_tfidf, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_per_topic=get_docs_per_topic(lsi_model)\n",
    "for i in range(10):\n",
    "    docs_per_topic[i].sort(reverse=True,key=itemgetter(1))\n",
    "get_answer_lsa('How to add a smooth transition in HTML with CSS?',docs_per_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = None\n",
    "n_out = 7\n",
    "n_weight = 5\n",
    "n_samp=10000\n",
    "n_components    = 10     \n",
    "n_clusters      = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [ [token for token in doc['clean_aa'].split(\" \") if token_frequency[token] > 1]\n",
    "                for doc in docs  ]\n",
    "tokenized   = [ ' '.join(doc) for doc in documents ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer  = TfidfVectorizer(max_df=0.9,  min_df=6,\n",
    "                            max_features=500, use_idf=True,\n",
    "                            strip_accents='ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ques_col.find({}, no_cursor_timeout=True).limit(n_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(tokenized)\n",
    "\n",
    "svd  = TruncatedSVD(n_components, random_state= 10)\n",
    "svdX = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=100, n_init=4,\n",
    "                verbose=False, random_state= 10)\n",
    "km.fit(svdX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.silhouette_score(svdX, km.labels_, sample_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lsa_topics(svd, terms, n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred  = km.predict(svdX)\n",
    "centers = km.cluster_centers_\n",
    "\n",
    "plot_clusters(svdX, y_pred, centers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
